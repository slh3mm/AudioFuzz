{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Audio\n",
    "import jiwer\n",
    "from IPython.display import Audio as IPyAudio, display\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset in Streaming Mode\n",
    "dataset = load_dataset(\n",
    "    \"openslr/librispeech_asr\", \n",
    "    \"clean\", \n",
    "    split=\"test\", \n",
    "    streaming=True, \n",
    "    trust_remote_code=True  \n",
    ")\n",
    "\n",
    "# Manually collect a subset of 5 samples for testing.\n",
    "subset = []\n",
    "for i, example in enumerate(dataset):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    subset.append(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration – Audio Playback & Visualization\n",
    "sample = subset[0]\n",
    "audio_data = sample[\"audio\"][\"array\"]\n",
    "sampling_rate = sample[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "display(IPyAudio(data=audio_data, rate=sampling_rate))\n",
    "\n",
    "# Plot the audio waveform.\n",
    "time_axis = np.linspace(0, len(audio_data) / sampling_rate, num=len(audio_data))\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(time_axis, audio_data)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Clean Audio Waveform\")\n",
    "plt.show()\n",
    "\n",
    "# Plot a spectrogram for additional insight.\n",
    "plt.figure(figsize=(14, 4))\n",
    "S = librosa.stft(audio_data)\n",
    "S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "librosa.display.specshow(S_db, sr=sampling_rate, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(\"Clean Audio Spectrogram\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Noise Injection Functions\n",
    "\n",
    "def apply_gaussian_noise(audio_array, intensity=1.0):\n",
    "    \"\"\"Add Gaussian noise scaled by intensity.\"\"\"\n",
    "    noise = np.random.normal(0, 0.02 * intensity, size=audio_array.shape)\n",
    "    return audio_array + noise\n",
    "\n",
    "def apply_realistic_noise(audio_array, intensity=1.0):\n",
    "    \"\"\"\n",
    "    Placeholder for realistic noise.\n",
    "    For now, we simulate it as a different Gaussian noise (could later be replaced by your fuzzer).\n",
    "    You might eventually add effects like pitch scaling or reverb.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, 0.03 * intensity, size=audio_array.shape)\n",
    "    return audio_array + noise\n",
    "\n",
    "def apply_noise(audio_array, noise_type=\"clean\", intensity=1.0):\n",
    "    \"\"\"\n",
    "    Apply noise based on noise_type:\n",
    "      - \"clean\": No noise is added.\n",
    "      - \"gaussian\": Add Gaussian noise.\n",
    "      - \"realistic\": Add realistic (fuzzed) noise.\n",
    "    \"\"\"\n",
    "    if noise_type == \"clean\":\n",
    "        return audio_array\n",
    "    elif noise_type == \"gaussian\":\n",
    "        return apply_gaussian_noise(audio_array, intensity)\n",
    "    elif noise_type == \"realistic\":\n",
    "        return apply_realistic_noise(audio_array, intensity)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown noise type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Metric Functions\n",
    "\n",
    "def compute_snr(clean, noisy):\n",
    "    \"\"\"Compute Signal-to-Noise Ratio (SNR) in dB.\"\"\"\n",
    "    noise = clean - noisy\n",
    "    snr = 10 * np.log10(np.sum(clean**2) / np.sum(noise**2) + 1e-10)\n",
    "    return snr\n",
    "\n",
    "def compute_mcd(clean, noisy, sr=16000, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Compute Mean Cepstral Distortion (MCD) between clean and noisy signals.\n",
    "    This is a rough approximation using MFCCs.\n",
    "    \"\"\"\n",
    "    mfcc_clean = librosa.feature.mfcc(y=clean, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_noisy = librosa.feature.mfcc(y=noisy, sr=sr, n_mfcc=n_mfcc)\n",
    "    diff = mfcc_clean - mfcc_noisy\n",
    "    # Euclidean distance per frame, averaged\n",
    "    dist = np.sqrt(np.sum(diff**2, axis=0))\n",
    "    mcd = (10.0 / np.log(10)) * np.sqrt(2) * np.mean(dist)\n",
    "    return mcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model and Processor\n",
    "model_name = \"openai/whisper-large-v3-turbo\"  # Change if desired\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Over Multiple Noise Conditions\n",
    "\n",
    "# Define the noise conditions and, optionally, a list of intensities for a sweep.\n",
    "noise_conditions = [\"clean\", \"gaussian\", \"realistic\"]\n",
    "intensity = 1.0  # For simplicity, use a single intensity value; you could loop over multiple values.\n",
    "\n",
    "# Initialize dictionaries to store metrics per noise condition.\n",
    "metrics = {noise: {\"ground_truths\": [], \"predictions\": [], \"snrs\": [], \"mcds\": []} \n",
    "           for noise in noise_conditions}\n",
    "\n",
    "for noise in noise_conditions:\n",
    "    print(f\"Evaluating noise condition: {noise}\")\n",
    "    for example in subset:\n",
    "        # Extract clean audio data.\n",
    "        audio = example[\"audio\"]\n",
    "        sampling_rate = audio.get(\"sampling_rate\", 16000)\n",
    "        clean_audio = audio[\"array\"]\n",
    "        \n",
    "        # Apply noise according to the condition.\n",
    "        noisy_audio = apply_noise(clean_audio, noise_type=noise, intensity=intensity)\n",
    "        \n",
    "        # Preprocess for model inference.\n",
    "        input_features = processor(noisy_audio, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        input_features = input_features.to(device)\n",
    "        \n",
    "        # Generate transcription.\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # Save ground truth and prediction.\n",
    "        metrics[noise][\"ground_truths\"].append(example[\"text\"])\n",
    "        metrics[noise][\"predictions\"].append(transcription)\n",
    "        \n",
    "        # Compute SNR and MCD (comparing noisy audio to clean audio).\n",
    "        snr_val = compute_snr(clean_audio, noisy_audio)\n",
    "        mcd_val = compute_mcd(clean_audio, noisy_audio, sr=sampling_rate)\n",
    "        metrics[noise][\"snrs\"].append(snr_val)\n",
    "        metrics[noise][\"mcds\"].append(mcd_val)\n",
    "        \n",
    "        # Print for inspection.\n",
    "        print(f\"Noise: {noise}\")\n",
    "        print(\"Ground Truth:\", example[\"text\"])\n",
    "        print(\"Prediction:  \", transcription)\n",
    "        print(f\"SNR: {snr_val:.2f} dB, MCD: {mcd_val:.2f}\")\n",
    "        print(\"-\" * 30)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Compute aggregate metrics using jiwer for transcription quality.\n",
    "results = {}\n",
    "for noise in noise_conditions:\n",
    "    wer_val = jiwer.wer(metrics[noise][\"ground_truths\"], metrics[noise][\"predictions\"])\n",
    "    cer_val = jiwer.cer(metrics[noise][\"ground_truths\"], metrics[noise][\"predictions\"])\n",
    "    avg_snr = np.mean(metrics[noise][\"snrs\"])\n",
    "    avg_mcd = np.mean(metrics[noise][\"mcds\"])\n",
    "    results[noise] = {\"WER\": wer_val, \"CER\": cer_val, \"Avg_SNR\": avg_snr, \"Avg_MCD\": avg_mcd}\n",
    "\n",
    "print(\"Aggregate Evaluation Results:\")\n",
    "for noise, res in results.items():\n",
    "    print(f\"{noise.capitalize()} Condition:\")\n",
    "    print(f\"  WER (word error rate): {res['WER']:.2%}\")\n",
    "    print(f\"  CER (character error rate): {res['CER']:.2%}\")\n",
    "    print(f\"  Average SNR (signal-to-noise ratio): {res['Avg_SNR']:.2f} dB\")\n",
    "    print(f\"  Average MCD (mean cepstral distortion): {res['Avg_MCD']:.2f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Qualitative Evaluation – Visualize Clean vs. Noisy Audio\n",
    "\n",
    "# Select one sample from the subset for a detailed qualitative comparison.\n",
    "sample = subset[0]\n",
    "clean_audio = sample[\"audio\"][\"array\"]\n",
    "sampling_rate = sample[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "# Generate noisy versions for each noise condition.\n",
    "noisy_versions = {noise: apply_noise(clean_audio, noise_type=noise, intensity=1.0) for noise in noise_conditions if noise != \"clean\"}\n",
    "noisy_versions[\"clean\"] = clean_audio  # Include clean audio for comparison\n",
    "\n",
    "# Display audio players for each condition.\n",
    "for noise, audio_version in noisy_versions.items():\n",
    "    print(f\"Audio for {noise.capitalize()} condition:\")\n",
    "    display(IPyAudio(data=audio_version, rate=sampling_rate))\n",
    "\n",
    "# Plot spectrograms side-by-side for visual comparison.\n",
    "fig, axes = plt.subplots(1, len(noisy_versions), figsize=(16, 4))\n",
    "for ax, (noise, audio_version) in zip(axes, noisy_versions.items()):\n",
    "    S = librosa.stft(audio_version)\n",
    "    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "    img = librosa.display.specshow(S_db, sr=sampling_rate, x_axis='time', y_axis='hz', ax=ax)\n",
    "    ax.set_title(f\"{noise.capitalize()} Audio\")\n",
    "    ax.label_outer()\n",
    "fig.colorbar(img, ax=axes, format='%+2.0f dB')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Metrics Summary Across Noise Conditions\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the aggregate results.\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results.index.name = \"Noise Condition\"\n",
    "print(df_results)\n",
    "\n",
    "# Plot a bar chart for WER and CER.\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "df_results[\"WER\"].plot(kind=\"bar\", ax=ax[0], color=\"skyblue\", title=\"WER by Noise Condition\")\n",
    "ax[0].set_ylabel(\"WER\")\n",
    "\n",
    "df_results[\"CER\"].plot(kind=\"bar\", ax=ax[1], color=\"salmon\", title=\"CER by Noise Condition\")\n",
    "ax[1].set_ylabel(\"CER\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
